{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjvS73GnXfuH",
        "outputId": "c12423b0-1e70-4633-b209-691ec9a128ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.8/runpy.py:127: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns \n",
        "import sklearn\n",
        "import regex as re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tag import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.tag import PerceptronTagger\n",
        "from collections import Counter\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "!python -m nltk.downloader punkt\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('../content/drive/MyDrive/Colab Notebooks/h_artists_songs.csv') "
      ],
      "metadata": {
        "id": "iNJ2xOrwXu-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lyrics = data['Lyrics'].tolist()"
      ],
      "metadata": {
        "id": "Nw91FXW9YZWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lyrics = [re.sub('\\s+', ' ', text) for text in lyrics]\n",
        "\n",
        "# removing non-English letters from lyrics\n",
        "lyrics = [re.sub('[^a-zA-Z0-9]+', ' ',text) for text in lyrics]\n",
        "\n",
        "# removing word 'Romanized' from lyrics' text\n",
        "lyrics = [re.sub('Romanized', ' ', text) for text in lyrics]\n",
        "\n"
      ],
      "metadata": {
        "id": "5FgZAWWuae8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part1"
      ],
      "metadata": {
        "id": "2DuX-z6QYDu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_lyrics = []\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "for song in lyrics:\n",
        "  tokenized_lyrics.extend(tokenizer.tokenize(song.lower()))\n",
        "\n",
        "counter = Counter(tokenized_lyrics)\n",
        "\n",
        "top_20_tokens = counter.most_common(20)\n",
        "\n",
        "# Print the top 20 most frequent tokens and their frequencies\n",
        "print(top_20_tokens)"
      ],
      "metadata": {
        "id": "3YD8lqY2c4ss",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ead508e-dc98-4525-c2c7-b188e1d49a1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('i', 398111), ('you', 305416), ('the', 285885), ('to', 169739), ('and', 166153), ('a', 153805), ('it', 140690), ('me', 136123), ('t', 128903), ('s', 121650), ('my', 108543), ('in', 103929), ('m', 87304), ('that', 84415), ('of', 78766), ('we', 76239), ('on', 75463), ('your', 72928), ('all', 63774), ('is', 56276)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_lyrics_no_stop = []\n",
        "stop_words = stopwords.words('english')\n",
        "for token in tokenized_lyrics:\n",
        "  if token not in stop_words:\n",
        "    tokenized_lyrics_no_stop.append(token)\n",
        "porter = PorterStemmer()\n",
        "stems = [porter.stem(word) for word in tokenized_lyrics_no_stop]\n",
        "top_20_stems = Counter(stems).most_common(20)\n",
        "print(top_20_stems)"
      ],
      "metadata": {
        "id": "yurZfL45jR7F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea28e931-64ce-4c4b-c9bb-55a1b48b048d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('love', 57557), ('know', 51731), ('like', 44941), ('oh', 41254), ('go', 34454), ('get', 32530), ('got', 30926), ('time', 29214), ('one', 28595), ('come', 28511), ('let', 27234), ('yeah', 26021), ('never', 25781), ('feel', 24545), ('want', 24459), ('see', 24286), ('make', 23450), ('la', 23307), ('way', 22793), ('say', 22412)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagger = nltk.PerceptronTagger()\n",
        "tags = tagger.tag(tokenized_lyrics_no_stop)\n",
        "\n",
        "verbs = [word.lower() for word, tag in tags if tag.startswith('V')]\n",
        "verb_counts = Counter(verbs)\n",
        "\n",
        "top_verbs = verb_counts.most_common(20)\n",
        "print(top_verbs)"
      ],
      "metadata": {
        "id": "K7-NqdR42GUu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50b0ff0c-d6a9-41d4-e520-8054c44eabfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('know', 39155), ('got', 29575), ('go', 27172), ('love', 22096), ('get', 21752), ('say', 18995), ('see', 17488), ('make', 16546), ('want', 16519), ('take', 16112), ('come', 15842), ('let', 13540), ('said', 9966), ('think', 9028), ('keep', 8800), ('need', 8724), ('feel', 7886), ('gone', 7385), ('tell', 7280), ('find', 7069)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Were you surprised by the results? Why or why not?##\n",
        "For the top 20 most frequent tokens, we were surprised that we got a lot of single letters such as “I”, “a”. We were hesitant whether to include the stop words. After discussing the interpretations of the results we got, we decided to keep the stop words. With the stop words, we can learn better what the most common tokens are in songs. And those tokens could also be the most common used in English sentences. The verb list later show more meaningful items such as know, go, love, say. These verbs may represent the most frequent used verbs by singers. "
      ],
      "metadata": {
        "id": "u_P0AjaVsbRn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part2"
      ],
      "metadata": {
        "id": "SbUdfQFUX8N5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data2022=data[data['Year']==2022].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "Uc7lbytq_xr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lyrics2022 = data2022['Lyrics'].tolist()\n",
        "\n",
        "lyrics2022 = [re.sub('\\s+', ' ', text) for text in lyrics2022]\n",
        "\n",
        "lyrics2022 = [re.sub('[^a-zA-Z0-9]+', ' ',text) for text in lyrics2022]\n",
        "\n",
        "lyrics2022 = [re.sub('Romanized', ' ', text) for text in lyrics2022]\n"
      ],
      "metadata": {
        "id": "Awvos04QBhi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_lyrics22 = []\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "for song in lyrics2022:\n",
        "  tokenized_lyrics22.extend(tokenizer.tokenize(song.lower()))\n",
        "\n",
        "counter22 = Counter(tokenized_lyrics22)\n",
        "\n",
        "top_20_tokens22 = counter22.most_common(20)\n",
        "\n",
        "# Print the top 20 most frequent tokens and their frequencies\n",
        "print(top_20_tokens22)"
      ],
      "metadata": {
        "id": "KUhR7uT1B4QC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04650d89-4250-49de-9edc-17a18a3c2a64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('i', 10305), ('you', 7270), ('the', 6473), ('to', 3746), ('it', 3563), ('and', 3241), ('a', 3231), ('t', 3193), ('me', 3046), ('my', 2732), ('s', 2634), ('in', 2501), ('m', 2331), ('we', 2088), ('that', 1955), ('of', 1777), ('all', 1743), ('your', 1670), ('on', 1591), ('don', 1468)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_lyrics_no_stop22 = []\n",
        "stop_words = stopwords.words('english')\n",
        "for token in tokenized_lyrics22:\n",
        "  if token not in stop_words:\n",
        "    tokenized_lyrics_no_stop22.append(token)\n",
        "porter22 = PorterStemmer()\n",
        "stems22 = [porter22.stem(word) for word in tokenized_lyrics_no_stop22]\n",
        "top_20_stems22 = Counter(stems22).most_common(20)\n",
        "print(top_20_stems22)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlERoP0mXlyR",
        "outputId": "d747cd11-542d-4168-aaad-7586b9c374c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('know', 1430), ('like', 1250), ('oh', 1244), ('love', 1106), ('go', 811), ('yeah', 783), ('get', 783), ('never', 741), ('one', 721), ('got', 709), ('let', 695), ('want', 654), ('time', 637), ('come', 631), ('way', 619), ('feel', 605), ('back', 596), ('take', 586), ('see', 538), ('wanna', 517)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagger22 = nltk.PerceptronTagger()\n",
        "tags22 = tagger22.tag(tokenized_lyrics_no_stop22)\n",
        "\n",
        "verbs22 = [word.lower() for word, tag in tags22 if tag.startswith('V')]\n",
        "verb_counts22 = Counter(verbs22)\n",
        "\n",
        "top_verbs22 = verb_counts22.most_common(20)\n",
        "print(top_verbs22)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbdEfYR9YB9w",
        "outputId": "997b4dcf-f134-4db2-d467-c6bb809b52ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('know', 1123), ('got', 673), ('go', 615), ('get', 457), ('want', 445), ('say', 428), ('take', 422), ('love', 411), ('let', 404), ('see', 382), ('make', 358), ('come', 311), ('keep', 281), ('think', 262), ('feel', 233), ('find', 225), ('tell', 215), ('said', 209), ('need', 209), ('run', 173)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##In a paragraph, compare your results from Part 2 to those from Part 1. How much overlap is there?##\n",
        "\n",
        "Even if we filtered by genre, the most common tokens are still the same, which made us think that the most common tokens are prevalent in all English songs regardless of genre or date. For the list of stems and verbs, love appears less. As in the genre of 2022, singers may not focus so much on romantic. It can be due to the war or the pandemic. "
      ],
      "metadata": {
        "id": "ezDWTFf_tGSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part3"
      ],
      "metadata": {
        "id": "1rA-5XUgfqlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Hagg=data[data[\"Artist_Name\"]=='Haggard, Merle'].reset_index(drop=True)\n",
        "Jr=data[data[\"Artist_Name\"]=='Hank Williams Jr.'].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "Anngk_dSKcm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lyrics_hagg = Hagg['Lyrics'].tolist()\n",
        "lyrics_hagg = [re.sub('\\s+', ' ', text) for text in lyrics_hagg]\n",
        "lyrics_hagg = [re.sub('[^a-zA-Z0-9]+', ' ',text) for text in lyrics_hagg]\n",
        "lyrics_hagg = [re.sub('Romanized', ' ', text) for text in lyrics_hagg]\n",
        "\n",
        "tokenized_lyrics_hagg = []\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "for song in lyrics_hagg:\n",
        "  tokenized_lyrics_hagg.extend(tokenizer.tokenize(song.lower()))\n",
        "counter_hagg = Counter(tokenized_lyrics_hagg)\n",
        "\n",
        "tokenized_lyrics_no_stop_hagg = []\n",
        "stop_words = stopwords.words('english')\n",
        "for token in tokenized_lyrics_hagg:\n",
        "  if token not in stop_words:\n",
        "    tokenized_lyrics_no_stop_hagg.append(token)\n",
        "porter_hagg = PorterStemmer()\n",
        "stems_hagg = [porter_hagg.stem(word) for word in tokenized_lyrics_no_stop_hagg]\n",
        "top_20_stems_hagg = Counter(stems_hagg).most_common(20)\n",
        "print(top_20_stems_hagg)\n",
        "\n",
        "stems_hagg=' '.join(stems_hagg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNCN020qYJnt",
        "outputId": "6ca4e480-4917-40aa-c3a9-6ff8c3ac4e76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('love', 892), ('time', 550), ('know', 520), ('old', 422), ('go', 394), ('got', 392), ('like', 383), ('come', 378), ('one', 372), ('way', 357), ('man', 342), ('home', 326), ('day', 307), ('never', 283), ('make', 283), ('back', 276), ('let', 271), ('blue', 266), ('good', 262), ('alway', 254)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lyrics_jr = Jr['Lyrics'].tolist()\n",
        "lyrics_jr = [re.sub('\\s+', ' ', text) for text in lyrics_jr]\n",
        "lyrics_jr = [re.sub('[^a-zA-Z0-9]+', ' ',text) for text in lyrics_jr]\n",
        "lyrics_jr = [re.sub('Romanized', ' ', text) for text in lyrics_jr]\n",
        "\n",
        "tokenized_lyrics_jr = []\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "for song in lyrics_jr:\n",
        "  tokenized_lyrics_jr.extend(tokenizer.tokenize(song.lower()))\n",
        "counter_jr = Counter(tokenized_lyrics_jr)\n",
        "\n",
        "tokenized_lyrics_no_stop_jr = []\n",
        "stop_words = stopwords.words('english')\n",
        "for token in tokenized_lyrics_jr:\n",
        "  if token not in stop_words:\n",
        "    tokenized_lyrics_no_stop_jr.append(token)\n",
        "porter_jr = PorterStemmer()\n",
        "stems_jr = [porter_jr.stem(word) for word in tokenized_lyrics_no_stop_jr]\n",
        "top_20_stems_jr = Counter(stems_jr).most_common(20)\n",
        "print(top_20_stems_jr)\n",
        "stems_jr=' '.join(stems_jr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwN2dbJuZMrC",
        "outputId": "4fdfa103-48ee-4ccb-c897-024bf6dd443c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('love', 767), ('know', 665), ('got', 650), ('like', 610), ('get', 506), ('go', 470), ('one', 405), ('caus', 404), ('time', 402), ('man', 366), ('gonna', 360), ('never', 360), ('well', 345), ('come', 324), ('make', 306), ('take', 304), ('heart', 298), ('old', 296), ('oh', 289), ('way', 289)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "tfidf_matrix1 = tfidf_vectorizer.fit_transform([stems_jr])\n",
        "tfidf_matrix2 = tfidf_vectorizer.transform([stems_hagg])\n",
        "\n",
        "cosine_similarities = cosine_similarity(tfidf_matrix1, tfidf_matrix2)\n",
        "\n",
        "cosine_similarities"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQhcUdqTZ_v3",
        "outputId": "a3287fc0-8f03-4fe0-dd8b-2df679b73379"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.93176232]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Discuss why the two songs were most alike. What is this analysis able to tell us?\n",
        "\n",
        "We got 0.93176, which suggests their songs are very similar based on the stems. Merle Haggard and Hank Williams Jr. are known for their storytelling abilities and often used their music to explore a range of topics related to life, love, and the human experience. From the top 20 stems of their songs, we can also see the \"love\" and \"know\" are very common. "
      ],
      "metadata": {
        "id": "Sn3UYoJKn724"
      }
    }
  ]
}